{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG_Classify.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVQzwlOmsEzOVBabWlB9L6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/einsteinxx/UCLA_BE223C_SPRING_2021/blob/main/VGG_Classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_KlcbCgFI4z"
      },
      "source": [
        "#VGG_CLASSIFY\n",
        "\n",
        "\n",
        "*   This function will take in an input file and ROI and process a 3 class classification \n",
        "*   The final output will be a Cancer, No Cancer decision for the ROI given\n",
        "\n",
        "USAGE:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-Zb4FwANg0e",
        "outputId": "c8b7df27-8250-46cb-b453-694da0e06511"
      },
      "source": [
        "import os\n",
        "import sys  #to set local import folder\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import gc  #debug memory leaks in matplotlib\n",
        "import csv #read in description files\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "#show model design parameters with torchsummary\n",
        "import torchsummary\n",
        "from torchsummary import summary\n",
        "from torch import FloatTensor\n",
        "from torch import tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "from torchvision import transforms  #get normalization functions\n",
        "\n",
        "#\n",
        "# Read Data from google drive\n",
        "#\n",
        "from google.colab import drive #for loading gdrive data\n",
        "from google.colab import files\n",
        "################################################################################\n",
        "#\n",
        "# Load data from google drive\n",
        "#\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#\n",
        "# SET LOCAL IMPORT DIRECTORY FOR .PY FILES\n",
        "#\n",
        "sys.path.append('/content/gdrive/My Drive/DBT_WORKSPACE/DBT_PY_FILES')\n",
        "!ls '/content/gdrive/My Drive/DBT_WORKSPACE/DBT_PY_FILES'\n",
        "#from CustomImageDataset import CustomImageDataset\n",
        "from VGG16 import VGG16\n",
        "\n",
        "\n",
        "# install dependencies not included by Colab\n",
        "# use pip3 to ensure compatibility w/ Google Deep Learning Images \n",
        "!pip3 install -q pydicom \n",
        "!pip3 install -q tqdm \n",
        "!pip3 install -q imgaug\n",
        "!pip3 install -q pickle5\n",
        "\n",
        "import pydicom #to read dicom files\n",
        "from pydicom import dcmread\n",
        "import pickle5 as pickle; #generic storage of image array\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Enable GPU, if present\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if (train_on_gpu):\n",
        "    !nvidia-smi -L\n",
        "    !nvidia-smi \n",
        "    dev=torch.device(\"cuda\")\n",
        "else:\n",
        "    print('GPU NOT FOUND!!! USING CPU INSTEAD!!!!!')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data_dir = '/content/gdrive/My Drive/DBT_DATA/IMG_ARRAYS'\n",
        "patch_dir = '/content/gdrive/My Drive/DBT_WORKSPACE/TRAINING_PATCHES' \n",
        "model_dir = '/content/gdrive/My Drive/BE223C_SPRING_2021/MODEL_SAVE'\n",
        "tensorboard_dir = '/content/gdrive/My Drive/BE223C_SPRING_2021/TENSORBOARD_SUMMARIES'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#from get_dirs import get_dirs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "CustomImageDataset.py  __pycache__  VGG16.py\n",
            "VGG16(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (vgg16_stack): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (13): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): ReLU(inplace=True)\n",
            "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (23): Flatten(start_dim=1, end_dim=-1)\n",
            "    (24): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Linear(in_features=4096, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 7.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 6.8MB/s \n",
            "\u001b[?25h  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "GPU NOT FOUND!!! USING CPU INSTEAD!!!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FolZzOZZ3d3G"
      },
      "source": [
        "def create_augmented_data(image_data,flip = 1,rot90=1,rot180=1,rot270=1,patchx =244,patchy=244):\n",
        "    #flip and rotate the data to make new data files\n",
        "\n",
        "    import numpy as np\n",
        "    \n",
        "    output90 = np.zeros([3,patchx,patchy])\n",
        "    output180 = np.zeros([3,patchx,patchy])\n",
        "    output270 = np.zeros([3,patchx,patchy])\n",
        "    outputflip = np.zeros([3,patchx,patchy])\n",
        "\n",
        "    print(np.shape(output90))\n",
        "\n",
        "    for ii in range(0,np.shape(image_data)[0]):\n",
        "        image90 = np.rot90(image_data[ii,:,:])\n",
        "        image180 = np.rot90(image_data[ii,:,:],k=2)\n",
        "        image270 = np.rot90(image_data[ii,:,:],k=3)\n",
        "        imageflip = np.fliplr(image_data[ii,:,:])\n",
        "        output90[ii,:,:] = image90\n",
        "        output180[ii,:,:] = image180\n",
        "        output270[ii,:,:] = image270\n",
        "        outputflip[ii,:,:] = imageflip\n",
        "    plt.imshow(output90[1,:,:])\n",
        "\n",
        "\n",
        "    return output90, output180, output270, outputflip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcmHj50GppvL"
      },
      "source": [
        "\n",
        "\n",
        "class TestImageDataset(): \n",
        "    def __init__(self, patch_data,file_count=1,transform=None, target_transform=None):\n",
        "        #self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.patch_data = patch_data\n",
        "\n",
        "        self.file_count = file_count\n",
        "\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    def image_normalize(self,image):\n",
        "        #replace with the more tensor friendly normalize once tensor shapes confirmed\n",
        "        image = image/65535.0\n",
        "        return image\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.file_count #len(self.file_list) #99 #len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image = self.patch_data\n",
        "        image.astype(float)\n",
        "\n",
        "        #fname = self.file_list[index]\n",
        "\n",
        "        #VERIFY THE IMAGES ARE ALL THE SAME 3x244x244\n",
        "        shapes = image.shape\n",
        "        assert (shapes[0] == 3),\"Image slice error: {0}\".format(fname)\n",
        "        assert (shapes[1] == 244), print('Image row error: ',fname)\n",
        "        assert (shapes[2] == 244), print('Image column error: ',fname)\n",
        "        #if (shapes[0] != 3 and shapes[1] != 244 and shapes[2]!= 244):\n",
        "\n",
        "\n",
        "        #Normalize the data to 0,1 from 2^16\n",
        "        image = self.image_normalize(image) #image/65535.0 #image_normalize(image)\n",
        "\n",
        "\n",
        "        output90, output180, output270, outputflip = create_augmented_data(image,flip = 1,rot90=1,rot180=1,rot270=1)\n",
        "\n",
        "\n",
        "        out_data = [output90,output180, output270, outputflip]\n",
        "\n",
        "\n",
        "        sample = {\"image\": out_data} #, \"label\": label}\n",
        "            #sample = file_name\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeceW__ZdtyS"
      },
      "source": [
        "def choose_tissue_area(img_data, patch_x,patch_y,slice_selection = 2,debug=0):\n",
        "\n",
        "\n",
        "    if (debug == 1):  print('slice chosen is ',slice_selection)\n",
        "    slice_data = img_data[slice_selection,:,:]\n",
        "\n",
        "    # convert the grayscale image to binary image\n",
        "    #look for anything >1 and assign that a 1 value. Expectation is that \n",
        "    #any area without attenuation will have 0 value -- adjust the lower val\n",
        "    #if blurred areas need to be brought in. Last 0 is just the default\n",
        "    #threshold type\n",
        "    ret,thresh = cv2.threshold(slice_data,1,1,0)\n",
        "\n",
        "\n",
        "    # calculate moments of binary image\n",
        "    M = cv2.moments(thresh)\n",
        "    # calculate x,y coordinate of center\n",
        "    center_x = int(M[\"m10\"] / M[\"m00\"])\n",
        "    center_y = int(M[\"m01\"] / M[\"m00\"])\n",
        "    if (debug == 1):  print('center is ', center_x,center_y)\n",
        "\n",
        "\n",
        "    # create a patch centered at the centroid location\n",
        "    patch_lr = center_x-int(patch_x/2),center_x+int(patch_x/2)\n",
        "    patch_tb = center_y-int(patch_y/2),center_y+int(patch_y/2)\n",
        "    #        indices = np.where(slice_data!= [0])\n",
        "    #        coordinates = zip(indices[0], indices[1])\n",
        "\n",
        "    slice_lower = slice_selection -1\n",
        "    slice_upper = slice_selection +2\n",
        "    patch_img = img_data[slice_lower:slice_upper,patch_tb[0]:patch_tb[1],\n",
        "                         patch_lr[0]:patch_lr[1]]\n",
        "\n",
        "    total_slices = np.shape(patch_img)\n",
        "    original_shape = np.shape(img_data)\n",
        "    assert (total_slices[0] == 3), \\\n",
        "        \"Slice was not 3 in Normal create{},{},{}--{}\".format(total_slices, \\\n",
        "                                                          slice_lower, slice_upper, original_shape)\n",
        "\n",
        "    if debug == 1:\n",
        "        plt.figure()\n",
        "        plt.imshow(thresh,cmap='bone') #img_data[slice_selection,:,:],cmap='bone')\n",
        "        plt.text(center_x,center_y,'.')\n",
        "        plt.text(patch_lr[0],patch_tb[0],'O')\n",
        "        plt.colorbar()\n",
        "\n",
        "\n",
        "        #### Draw annotation box\n",
        "        xcorner = int(patch_lr[0])\n",
        "        ycorner = int(patch_tb[0])\n",
        "        xwidth = int(244)\n",
        "        yheight = int(244)\n",
        "\n",
        "        rect = patches.Rectangle((xcorner,ycorner),xwidth,yheight,linewidth=2,edgecolor='r',facecolor='none')\n",
        "        # Get the current reference\n",
        "        ax = plt.gca()\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    return patch_img,center_x,center_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iklCxIye-Da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9bf04637-14a8-41c7-d75c-aee6bf3383a9"
      },
      "source": [
        "####################################################\n",
        "# GENERATE RANDOM PATCHES FROM ARGUMENT BOX DATA\n",
        "####################################################\n",
        "\n",
        "def VGG_Classify(img_data, box_info,model_file):\n",
        "\n",
        "\n",
        "    from VGG16 import VGG16\n",
        "    model_vgg16 = VGG16() #.to(device)\n",
        "    model_vgg16 = model_vgg16.float()\n",
        "    print(model_vgg16)\n",
        "\n",
        "\n",
        "\n",
        "    ############################################################################\n",
        "    # LOAD MODEL FROM ARGUMENTS\n",
        "    checkpoint = torch.load(final_file, map_location=torch.device('cpu'))\n",
        "\n",
        "\n",
        "    model_vgg16.load_state_dict(checkpoint)\n",
        "    model_vgg16.eval()\n",
        "\n",
        "\n",
        "    ############################################################################\n",
        "    # Parse through argument box info\n",
        "    ############################################################################\n",
        "    start_point  = -1\n",
        "    patch_x = 244\n",
        "    patch_y = 244\n",
        "\n",
        "    #box_info = ['testfile',7,1800,125,2200,80]\n",
        "    row_corner = box_info[2]\n",
        "    col_corner = box_info[4]\n",
        "    box_slice = box_info[1]\n",
        "\n",
        "    roi_delta = 0.1\n",
        "\n",
        "\n",
        "    #This is the allowed range of box centers, all within the given ROI\n",
        "    selectable_col = range(box_info[4],box_info[4]+box_info[5])\n",
        "    selectable_row = range(box_info[2],box_info[2]+box_info[3])\n",
        "\n",
        "    desired_boxes = 50\n",
        "    box_safe =0\n",
        "    box_corners = []\n",
        "    counter = 0\n",
        "    while ((box_safe < 5) and (counter < 1e3)):\n",
        "        counter +=1 #safeguard for while\n",
        "\n",
        "        #generate a random point within or some % outside of the box limits\n",
        "        row_corner=np.random.randint(row_corner-row_corner*roi_delta, row_corner+ row_corner*roi_delta)\n",
        "        lowcol = col_corner-col_corner*roi_delta\n",
        "        highcol = col_corner+ col_corner*roi_delta\n",
        "        if (lowcol == highcol):\n",
        "            print('!!!!! Found identical corners!!!')\n",
        "        col_corner=np.random.randint(lowcol,highcol )\n",
        "        #check to see if this new corner extends beyond any image end\n",
        "        if ((row_corner < 0) or (col_corner <0) or \n",
        "            (((row_corner + patch_y) > irows) or ((col_corner + patch_x) > icols))):\n",
        "            #box will extend off image boundary\n",
        "            #don't let this new bounding box through, redo it\n",
        "            #print('UNSAFE BOX--REDO ',row_corner,col_corner)\n",
        "            pass\n",
        "        else: #This box is safe, use it for calculations\n",
        "            print(row_corner,col_corner, counter)\n",
        "            box_safe += 1\n",
        "            box_corners.append([row_corner, col_corner])\n",
        "\n",
        "    if (len(box_corners) != 5):\n",
        "        print('Failed to find enough sample boxes')\n",
        "\n",
        "    slice_lower = box_slice - 1\n",
        "    slice_upper = box_slice + 1\n",
        "\n",
        "\n",
        "    #build patches from random boxes\n",
        "    data = {}\n",
        "    for ii in range(0,1): #len(box_corners)):\n",
        "        row_lims_low, row_lims_high = box_corners[ii][0],box_corners[ii][0]+patch_y\n",
        "        col_lims_low, col_lims_high = box_corners[ii][1],box_corners[ii][1]+patch_x\n",
        "        patch_img = raw_image[slice_lower:slice_upper+1,row_lims_low:row_lims_high,col_lims_low:col_lims_high]\n",
        "\n",
        "        #Do Test Portion here\n",
        "        #1. Take in random patch, use transforms to make rotations and a flip\n",
        "        #2. Return those as the batch images\n",
        "        #3. Run Model over that set\n",
        "        #4. Take any that match cancer as the label\n",
        "\n",
        "        #load up with the pre-sized patch images\n",
        "        all_data = TestImageDataset(patch_data = patch_img,\n",
        "                                    file_count=1, #full_file_count,\n",
        "                                    transform=None, \n",
        "                                    target_transform=None)\n",
        "\n",
        "        #del patch_img\n",
        "        \n",
        "        dataloader_all = DataLoader(all_data, batch_size=1,shuffle=True, num_workers=2)#, \n",
        "        \n",
        "        count = 0\n",
        "        for ii in dataloader_all:\n",
        "            print(type(ii))\n",
        "            print(ii.keys())\n",
        "            key = list(ii.keys())\n",
        "            \n",
        "            plt.figure()\n",
        "            temp = ii[key[0]][count]\n",
        "            print('len of temp is ', len(temp))\n",
        "            plt.imshow(temp[0,0,:,:])\n",
        "\n",
        "            count +=1\n",
        "        print('counter is at ', count)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    total_accuracy = []\n",
        "    total_precision = []\n",
        "    total_average_precision=[] \n",
        "    total_final_accuracy = []\n",
        "    total_final_precision = []\n",
        "    total_final_recall_sens = []\n",
        "    total_f1_score =[]\n",
        "    total_TP = []\n",
        "    total_FP = []\n",
        "    total_TN = []\n",
        "    total_FN = []\n",
        "\n",
        "    stored_predictions = [] #keep all of the predictions \n",
        "    for epoch in range(0,1):\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(dataloader_all, 0):\n",
        "                print(i)\n",
        "                # get the inputs; data is a list of [inputs, labels]\n",
        "                #inputs, labels = data\n",
        "                print('len of data is ', len(data))\n",
        "                print('type of data is ', type(data))\n",
        "                num_images = len(data['image'])\n",
        "                for ii in range(0,num_images):\n",
        "                    inputs = data['image'][ii].type(FloatTensor)\n",
        "                    #labels = data['label'] #.type(FloatTensor)\n",
        "\n",
        "                    if (train_on_gpu):\n",
        "                        inputs = inputs.to(dev)\n",
        "\n",
        "                    # forward + backward + optimize\n",
        "                    outputs = model_vgg16(inputs) #.permute(0, 1, 2, 3))\n",
        "\n",
        "                    outputs=torch.flatten(outputs, start_dim=1)\n",
        "                    #loss = criterion(outputs, labels.long())\n",
        "\n",
        "\n",
        "                    #print(outputs)\n",
        "                    y_pred_softmax = torch.log_softmax(outputs, dim = 1)\n",
        "                    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
        "                    print('Predicted Value = ',ii,y_pred_tags)\n",
        "                    stored_predictions.append(y_pred_tags)\n",
        "\n",
        "\n",
        "    final_prediction = 0 #default is no cancer label\n",
        "    for ii in range(0,len(stored_predictions)):\n",
        "        if (stored_predictions[ii] == 2):\n",
        "            final_prediction = 1\n",
        "            break\n",
        "\n",
        "    return final_prediction\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG16(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (vgg16_stack): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (13): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): ReLU(inplace=True)\n",
            "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (23): Flatten(start_dim=1, end_dim=-1)\n",
            "    (24): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Linear(in_features=4096, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "shape of raw input image is  110 1900 2500\n",
            "1654 2216 503\n",
            "1473 2192 514\n",
            "890 2210 573\n",
            "931 2056 574\n",
            "970 2190 575\n",
            "(3, 244, 244)\n",
            "<class 'dict'>\n",
            "dict_keys(['image'])\n",
            "len of temp is  1\n",
            "counter is at  1\n",
            "(3, 244, 244)\n",
            "0\n",
            "len of data is  1\n",
            "type of data is  <class 'dict'>\n",
            "Predicted Value =  0 tensor([0])\n",
            "Predicted Value =  1 tensor([0])\n",
            "Predicted Value =  2 tensor([0])\n",
            "Predicted Value =  3 tensor([0])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANSklEQVR4nO3aQYxd5XnG8f9zx4OjEipwSS3LWIVE3tBFiWURpKKIijYBb0w2CBbFipDcBUiJ1C6cZhE2kdJKSSWkFskRKKZKoUgJwgvahlqRUBcQTESMgQJTAsKWsZtSEdRIDthvF/fMzJ3xDDOeO9f3jr7/T7o+33nPd859fe7Mo3PO3FQVktrVG3cDksbLEJAaZwhIjTMEpMYZAlLjDAGpcSMLgSS3JXk9yUySA6N6H0nDySi+J5BkCngD+DPgBPACcHdVvbrubyZpKKO6ErgRmKmqt6rqt8DjwN4RvZekIWwa0XG3A+8OrJ8AvrDc5MuyuT7F5f2VzP1DkvlJs/UsKCxcX9U2YPC4QC03b8Ex5verpbYtWv/EY3blWrR+UdsHaqvp58K+Vj7uRR17br2Wn7O4lkXzueCjgdSFbzWw38LTPH+srKI2+2nOvmeoRR/3/LwF+2R27sIeMnesWlhjoDbw/r25/S/sZ8ljD/w/BrenOykLt2fR/uHFY2d/VVWfYZFRhcCKkuwH9gN8it/hC70/hfRIL5Ae9EKmpvqfeK/XD4ReYLY2Oyfpr8/N7dYTaqo3Nx7cVr1e/xpodl7SX+/1+r8kva4WqNnj9ZivJVRvdh7dMVg4p8fCuWHh3G7O3Hhgv/kac5/ycnMvmDdQH9y25DHnegZSF7zfXBDN1WqZY9TA+vyYpca97helW8/sstf/Ie+vV/exF8nsi7nxVObrva4+1TtPrwuMXqo/TrGpd54es3P7r005Ty/n59Z7KXoUm3rnmOrG/fp5plJsyjl6KaaY32c655hK/9iDy+mc6+9Htz/nB+b2j9c/znkuy7mu1s3vxtM5xxSDxyim5+ZUd4yiB0wHpoCphB4wRZhOjx69rtajR5hKj6ltM+8s9bs4qtuBk8COgfVrutqcqjpYVburavc0m0fUhqSVjCoEXgB2JrkuyWXAXcDhEb2XpCGM5Hagqj5Ocj/wb/SvVh6pqldG8V6ShjOyZwJV9TTw9KiOL2l9+I1BqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxhoDUOENAapwhIDXOEJAaZwhIjTMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxhoDUOENAapwhIDVu0zA7J3kb+BA4B3xcVbuTbAH+GbgWeBu4s6r+d7g2JY3KelwJ/ElV3VBVu7v1A8CRqtoJHOnWJU2oUdwO7AUOdeNDwB0jeA9J62TYECjgJ0leTLK/q22tqlPd+D1g61I7Jtmf5GiSox9xdsg2JK3VUM8EgJur6mSS3weeSfKfgxurqpLUUjtW1UHgIMDvZsuScySN3lBXAlV1slueAZ4EbgROJ9kG0C3PDNukpNFZcwgkuTzJFbNj4EvAceAwsK+btg94atgmJY3OMLcDW4Enk8we55+q6l+TvAA8keRe4B3gzuHblDQqaw6BqnoL+KMl6v8D3DpMU5IuHb8xKDXOEJAaZwhIjTMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxhoDUOENAapwhIDXOEJAaZwhIjTMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGmcISI1bMQSSPJLkTJLjA7UtSZ5J8ma3vKqrJ8mDSWaSHEuya5TNSxreaq4EfgDctqh2ADhSVTuBI906wO3Azu61H3hofdqUNCorhkBVPQu8v6i8FzjUjQ8BdwzUH62+54Ark2xbr2Ylrb+1PhPYWlWnuvF7wNZuvB14d2Deia4maUIN/WCwqgqoi90vyf4kR5Mc/Yizw7YhaY3WGgKnZy/zu+WZrn4S2DEw75qudoGqOlhVu6tq9zSb19iGpGGtNQQOA/u68T7gqYH6Pd1fCW4CPhi4bZA0gTatNCHJY8AtwNVJTgDfAr4DPJHkXuAd4M5u+tPAHmAG+A3w1RH0LGkdrRgCVXX3MptuXWJuAfcN25SkS8dvDEqNMwSkxhkCUuMMAalxhoDUOENAapwhIDXOEJAaZwhIjTMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxhoDUOENAapwhIDXOEJAaZwhIjTMEpMYZAlLjDAGpcSuGQJJHkpxJcnyg9kCSk0le6l57BrZ9I8lMkteTfHlUjUtaH6u5EvgBcNsS9b+rqhu619MASa4H7gL+sNvnH5JMrVezktbfiiFQVc8C76/yeHuBx6vqbFX9EpgBbhyiP0kjNswzgfuTHOtuF67qatuBdwfmnOhqF0iyP8nRJEc/4uwQbUgaxlpD4CHgc8ANwCnguxd7gKo6WFW7q2r3NJvX2IakYa0pBKrqdFWdq6rzwPeZv+Q/CewYmHpNV5M0odYUAkm2Dax+BZj9y8Fh4K4km5NcB+wEfjZci5JGadNKE5I8BtwCXJ3kBPAt4JYkNwAFvA38BUBVvZLkCeBV4GPgvqo6N5rWJa2HFUOgqu5eovzwJ8z/NvDtYZqSdOn4jUGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxhoDUOENAapwhIDXOEJAaZwhIjTMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0Bq3IohkGRHkp8meTXJK0m+1tW3JHkmyZvd8qquniQPJplJcizJrlH/JySt3WquBD4G/rKqrgduAu5Lcj1wADhSVTuBI906wO3Azu61H3ho3buWtG5WDIGqOlVVP+/GHwKvAduBvcChbtoh4I5uvBd4tPqeA65Msm3dO5e0Li7qmUCSa4HPA88DW6vqVLfpPWBrN94OvDuw24muJmkCrToEknwa+BHw9ar69eC2qiqgLuaNk+xPcjTJ0Y84ezG7SlpHqwqBJNP0A+CHVfXjrnx69jK/W57p6ieBHQO7X9PVFqiqg1W1u6p2T7N5rf1LGtJq/joQ4GHgtar63sCmw8C+brwPeGqgfk/3V4KbgA8GbhskTZhNq5jzx8CfAy8neamr/TXwHeCJJPcC7wB3dtueBvYAM8BvgK+ua8eS1tWKIVBV/wFkmc23LjG/gPuG7EvSJeI3BqXGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxhoDUOENAapwhIDXOEJAaZwhIjTMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuNSVePugST/Dfwf8Ktx93IRrmZj9Qsbr+eN1i9Mds9/UFWfWVyciBAASHK0qnaPu4/V2mj9wsbreaP1CxuzZ28HpMYZAlLjJikEDo67gYu00fqFjdfzRusXNmDPE/NMQNJ4TNKVgKQxGHsIJLktyetJZpIcGHc/y0nydpKXk7yU5GhX25LkmSRvdsurxtjfI0nOJDk+UFuyv/Q92J3zY0l2TVDPDyQ52Z3nl5LsGdj2ja7n15N8eQz97kjy0ySvJnklyde6+kSf5xVV1dhewBTwX8BngcuAXwDXj7OnT+j1beDqRbW/BQ504wPA34yxvy8Cu4DjK/UH7AH+BQhwE/D8BPX8APBXS8y9vvv52Axc1/3cTF3ifrcBu7rxFcAbXV8TfZ5Xeo37SuBGYKaq3qqq3wKPA3vH3NPF2Asc6saHgDvG1UhVPQu8v6i8XH97gUer7zngyiTbLk2n85bpeTl7gcer6mxV/RKYof/zc8lU1amq+nk3/hB4DdjOhJ/nlYw7BLYD7w6sn+hqk6iAnyR5Mcn+rra1qk514/eAreNpbVnL9Tfp5/3+7vL5kYFbrInqOcm1wOeB59m45xkYfwhsJDdX1S7gduC+JF8c3Fj967+J/VPLpPc34CHgc8ANwCngu+Nt50JJPg38CPh6Vf16cNsGOs9zxh0CJ4EdA+vXdLWJU1Unu+UZ4En6l6KnZy/vuuWZ8XW4pOX6m9jzXlWnq+pcVZ0Hvs/8Jf9E9Jxkmn4A/LCqftyVN9x5HjTuEHgB2JnkuiSXAXcBh8fc0wWSXJ7kitkx8CXgOP1e93XT9gFPjafDZS3X32Hgnu7p9U3ABwOXs2O16J75K/TPM/R7vivJ5iTXATuBn13i3gI8DLxWVd8b2LThzvMC434ySf8J6hv0n/Z+c9z9LNPjZ+k/mf4F8Mpsn8DvAUeAN4F/B7aMscfH6F8+f0T/3vPe5fqj/7T677tz/jKwe4J6/seup2P0f4m2Dcz/Ztfz68DtY+j3ZvqX+seAl7rXnkk/zyu9/Mag1Lhx3w5IGjNDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXH/D5hiY4nk26zfAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kc0Y8k-e7Rop",
        "outputId": "f9e99d22-380e-4c9a-ead4-2edd29094395"
      },
      "source": [
        "\n",
        "for i, data in enumerate(dataloader_all, 0):\n",
        "    print('data size is ', len(data))\n",
        "    print('data key = ', data.keys())\n",
        "    \n",
        "    print('i, size of data is ', i,(np.shape(torch.squeeze(data['image'][0]))))\n",
        "    print('list inside keyed dict = ',len(data['image']))\n",
        "    for jj in range(0,len(data['image'])):\n",
        "        idata = torch.squeeze(data['image'][jj])\n",
        "\n",
        "        plt.figure()\n",
        "        plt.imshow(idata[0,:,:])\n",
        "        plt.title(str(jj))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 244, 244)\n",
            "data size is  1\n",
            "data key =  dict_keys(['image'])\n",
            "i, size of data is  0 torch.Size([3, 244, 244])\n",
            "list inside keyed dict =  4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0dCHWoxmWei"
      },
      "source": [
        "#\n",
        "# Read inputs\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzVhXmL_h-uK"
      },
      "source": [
        "################################################################################\n",
        "# GET FULL LIST OF FILES IN IMAGE ARRAY DIRECTORY\n",
        "################################################################################\n",
        "use_patch_files = 1\n",
        "if (use_patch_files == 0):\n",
        "    raw_files = os.listdir(data_dir)\n",
        "    print('found #files: ',len(raw_files))\n",
        "else: #patches broken up into directories\n",
        "    category_folders = os.listdir(patch_dir)\n",
        "    #raw_files = os.listdir(data_dir)\n",
        "    #print('found #files: ',len(raw_files))    \n",
        "\n",
        "if (0):\n",
        "    #create fake patches for now\n",
        "    patch_dict = {}\n",
        "    for counter,filename in enumerate(raw_files):\n",
        "        #load full array\n",
        "        full_filename = os.path.join(data_dir,filename)\n",
        "        img_data = pickle.load( open( full_filename, \"rb\" ) )\n",
        "        patch_data = img_data[0:3,:,:]\n",
        "        patch_dict[counter]= patch_data\n",
        "        print(full_filename,np.shape(patch_data))\n",
        "\n",
        "        if (counter > 0):\n",
        "            break\n",
        "        \n",
        "### SKIP ACTIONABLE FOLDER\n",
        "### Remove actionable folder item from list, since we're no longer using that data\n",
        "category_folders.remove('ACTIONABLE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cHnebpfg1Kt"
      },
      "source": [
        "for ii in category_folders:\n",
        "    print(ii)\n",
        "    flist = os.listdir(os.path.join(patch_dir,ii))\n",
        "\n",
        "temp = flist[0].split(sep='_')\n",
        "print(temp[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09AcXoOggwmF"
      },
      "source": [
        "#generate full file list for use in indexing the dataloader\n",
        "#this replaces the older loader, which was only inputting Normal\n",
        "full_file_list = [] #store the full filename of every file\n",
        "full_category_name = []\n",
        "for category_folder in category_folders:\n",
        "            print('------------- ',category_folder)\n",
        "            file_list = os.listdir(os.path.join(patch_dir,category_folder))\n",
        "            cat_count = 0\n",
        "            for file_name in file_list:\n",
        "                cat_count = cat_count + 1\n",
        "                full_category_name.append(category_folder)\n",
        "                full_file_list.append(file_name)\n",
        "                print(os.path.join(category_folder,file_name))\n",
        "full_file_count = len(full_file_list)\n",
        "print(len(full_file_list))\n",
        "full_file_list[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_kQ8Ao9UR1o"
      },
      "source": [
        "#REPLACE CUSTOMIMAGEDATASET WITH .PY ONCE COMPLETE!!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJHlvoDZUOJ0"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms  #get normalization functions\n",
        "\n",
        "class TestImageDataset(): #Dataset):\n",
        "    def __init__(self, img_dir,category=[],file_count=1,file_list =[],transform=None, target_transform=None):\n",
        "        #self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.category = category\n",
        "        self.file_count = file_count\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.category_name =''\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    def image_normalize(image):\n",
        "        #replace with the more tensor friendly normalize once tensor shapes confirmed\n",
        "        image = image/65535.0\n",
        "        return image\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.file_count #len(self.file_list) #99 #len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        fname = self.file_list[index]\n",
        "\n",
        "        #get label and pull category \n",
        "        text_tokens = fname.split(sep='_')\n",
        "        print('text tokens ',text_tokens)\n",
        "        label_class = text_tokens[3] #get the label token in 4th position\n",
        "        self.category_name =  label_class.upper() \n",
        "\n",
        "\n",
        "        full_file_name = os.path.join(self.img_dir,self.category_name,fname)\n",
        "        image = pickle.load( open( full_file_name, \"rb\" ) )\n",
        "        image = image.astype(float) #using patch images\n",
        "        \n",
        "\n",
        "        #VERIFY THE IMAGES ARE ALL THE SAME 3x244x244\n",
        "        shapes = image.shape\n",
        "        assert (shapes[0] == 3),\"Image slice error: {0}\".format(fname)\n",
        "        assert (shapes[1] == 244), print('Image row error: ',fname)\n",
        "        assert (shapes[2] == 244), print('Image column error: ',fname)\n",
        "        #if (shapes[0] != 3 and shapes[1] != 244 and shapes[2]!= 244):\n",
        "\n",
        "\n",
        "        #Normalize the data to 0,1 from 2^16\n",
        "        image = image/65535.0 #image_normalize(image)\n",
        "\n",
        "\n",
        "\n",
        "        #test out numeric label\n",
        "        if (label_class in 'Normal'):\n",
        "            label = 0\n",
        "        elif (label_class in 'Actionable'):\n",
        "            print('!!!!! ACTIONABLE PASSED THROUGH')\n",
        "            stop()\n",
        "            label = 1\n",
        "        elif (label_class in 'Benign'):\n",
        "            label = 1\n",
        "        else: # (label_class in 'Cancer'):\n",
        "            label = 2\n",
        "\n",
        "\n",
        "        #print(full_file_name)\n",
        "\n",
        "\n",
        "        #read_image(img_path)\n",
        "        #label = self.img_labels.iloc[idx, 1]\n",
        "        #if self.transform:\n",
        "        #    image = self.transform(image)\n",
        "        #if self.target_transform:\n",
        "        #    label = self.target_transform(label)\n",
        "        sample = {\"image\": image} #, \"label\": label}\n",
        "            #sample = file_name\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtQmahA8p7U6"
      },
      "source": [
        "def calculate_metrics(labels, y_pred_tags):\n",
        "    #calculate PRECISION and other metrics to guage the model results\n",
        "    #get # of true cancer cases, find # of cancer cases falsely predicted\n",
        "    from sklearn.metrics import average_precision_score\n",
        "\n",
        "    temp_labels = []\n",
        "    temp_prediction = []\n",
        "    true_pos =[]\n",
        "\n",
        "\n",
        "    #initialize variables for manual calculations\n",
        "    TP = 0 #true positive\n",
        "    FP = 0 #false positive\n",
        "    TN = 0 #true negative\n",
        "    FN = 0 #false negative\n",
        "    BP = 0 #benign positive match\n",
        "    BN = 0 #benign negative match\n",
        "    num0=0\n",
        "    num1=0\n",
        "    num2=0\n",
        "    for i in range(len(labels)): \n",
        "        if y_pred_tags[i]==labels[i]==2:\n",
        "            TP += 1\n",
        "\n",
        "        #FP\n",
        "        if y_pred_tags[i]==2 and labels[i]!=y_pred_tags[i]:\n",
        "            FP += 1\n",
        "        #if y_pred_tags[i]==1 and labels[i]!=y_pred_tags[i]:\n",
        "        #    FP += 1\n",
        "\n",
        "        #TN\n",
        "        if labels[i]==y_pred_tags[i]==0:\n",
        "            TN += 1\n",
        "        if labels[i]==y_pred_tags[i]==1:\n",
        "            TN += 1\n",
        "\n",
        "        #FN\n",
        "        if y_pred_tags[i]==0 and labels[i]==2: #!=y_pred_tags[i]:\n",
        "            FN += 1\n",
        "        if y_pred_tags[i]==1 and labels[i]==2: #!=y_pred_tags[i]:\n",
        "            FN += 1\n",
        "\n",
        "        #check to see how benign does\n",
        "        if (y_pred_tags[i] ==1 and labels[i] != 1):\n",
        "            BN +=1\n",
        "        if (y_pred_tags[i] ==1 and labels[i] == 1):\n",
        "            BP +=1\n",
        "\n",
        "        if (y_pred_tags[i]==0):\n",
        "            num0+=1\n",
        "        elif (y_pred_tags[i] == 1):\n",
        "            num1+=1\n",
        "        else:\n",
        "            num2 +=1\n",
        "\n",
        "    #F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
        "    print('Predicted Matches #0,#1,#2 = ',num0,num1,num2)\n",
        "    print('BN Match = ',BN,BP)\n",
        "    final_accuracy = (TP+TN)/(TP+FP+FN+TN +1e-12)\n",
        "    print('Accuracy is ',final_accuracy)\n",
        "    final_precision = TP/(TP+FP + 1e-12)\n",
        "    print('Precision is ',final_precision)\n",
        "    final_recall_sens = TP/(TP + FN + 1e-12)\n",
        "    print('Recall/Sens = ',final_recall_sens)\n",
        "    f1_score = 2 *(final_recall_sens * final_precision)/(final_recall_sens + final_precision +1e-12)\n",
        "    print('F1 score = ',f1_score)\n",
        "\n",
        "\n",
        "    #########################################################################\n",
        "    # Consolidate the 3 classes into a binary setup. Cancer label is 1, \n",
        "    #benign and no cancer are lumped into class 0\n",
        "    #########################################################################\n",
        "    for ii in labels:\n",
        "        if (ii == 2):\n",
        "            temp_labels.append(1)\n",
        "        else:\n",
        "            temp_labels.append(0)\n",
        "\n",
        "    for ii in y_pred_tags:\n",
        "        if (ii == 2):\n",
        "            temp_prediction.append(1)\n",
        "        else:\n",
        "            temp_prediction.append(0)\n",
        "    print(temp_labels)\n",
        "    print(temp_prediction) \n",
        "    print(labels)\n",
        "    print(y_pred_tags)                  \n",
        "    average_precision = average_precision_score(temp_labels, temp_prediction)\n",
        "\n",
        "    print('Average precision-recall score: {0:0.2f}'.format(\n",
        "        average_precision))\n",
        "    \n",
        "    scores = [TP, FP,TN,FN]\n",
        "    return average_precision, final_accuracy, final_precision, final_recall_sens, f1_score, scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMDx0HNSQY6I"
      },
      "source": [
        "#Show summary of model setup and move model to the GPU\n",
        " #train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "bsize = 50\n",
        "\n",
        "model_vgg16 = VGG16() #.to(device)\n",
        "model_vgg16 = model_vgg16.float()\n",
        "print(model_vgg16)\n",
        "\n",
        "\n",
        "if (train_on_gpu == 1):\n",
        "    #dev=torch.device(\"cuda\") \n",
        "    model_vgg16.to(dev)\n",
        "    summary(model_vgg16,(3,244,244), batch_size = bsize, device='cuda')\n",
        "else:\n",
        "    summary(model_vgg16,(3,244,244), batch_size = bsize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opgp3puefO2S"
      },
      "source": [
        "#\n",
        "# LOSS FUNCTION SETUP\n",
        "#\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "\n",
        "import torch\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #weight = weight_numbers)\n",
        "if (train_on_gpu ==1):\n",
        "    criterion.cuda(dev)\n",
        "\n",
        "#OPTIMIZERS\n",
        "#optimizer = optim.SGD(model_vgg16.parameters(), lr=0.1, momentum = 0.99) #lr=0.001, momentum=0.9)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "optimizer = torch.optim.Adam(model_vgg16.parameters(), lr=0.000007, weight_decay=0.00000)#lr=0.00005, weight_decay=0.0000)\n",
        "\n",
        "#add scheduler to adjust learning rates when val gets stuck\n",
        "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.1)\n",
        "\n",
        "if (train_on_gpu == 1):\n",
        "    m = nn.LogSoftmax(dim=1).cuda(dev)\n",
        "    nll_loss = nn.NLLLoss().cuda(dev)\n",
        "else:\n",
        "    m = nn.LogSoftmax(dim=1)\n",
        "    nll_loss = nn.NLLLoss()\n",
        "\n",
        "L1loss = nn.L1Loss()\n",
        "\n",
        "model_vgg16.parameters\n",
        "#summary(model_vgg16, (3, 224, 224))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iod5upiXOAzv"
      },
      "source": [
        "#\n",
        "# Generate Random image for loading\n",
        "#\n",
        "raw_image = np.random.rand(110,1900,2500)\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(raw_image[50,:,:],cmap='jet')\n",
        "plt.colorbar()\n",
        "\n",
        "#\n",
        "# PARSE BOX LOCATIONS\n",
        "#\n",
        "box = [13,250,159,2100,180]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6wot0haE0ox"
      },
      "source": [
        "model_name = 'vgg16_best_accuracy_93_EPOCH_96_0.04582521319389343'\n",
        "index_storage_file = os.path.join(tensorboard_dir,'data_index_052521_gpu.pickle')\n",
        "index_file = os.path.join(tensorboard_dir,'data_index_052521_gpu.pickle') #052021_gpu.pickle') #data_index_last.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFiR_Jx2GgQe"
      },
      "source": [
        "### BYPASS DATALOADER TO MODEL AND TRY A SINGLE FILE\n",
        "\n",
        "final_file = os.path.join(model_dir,model_name) #'vgg16_best_accuracy_81_EPOCH_79') #95_gpu_051821')#'vgg16_best_accuracy_97_gpu_final')\n",
        "checkpoint = torch.load(final_file, map_location=torch.device('cpu'))\n",
        "\n",
        "\n",
        "use_index = 1\n",
        "if (use_index == 1):\n",
        "    #[val_index, test_index, file_list]\n",
        "    validation_saved_index,test_saved_index,saved_flist=pickle.load( open( index_file, \"rb\" )) \n",
        "    test_files=[]\n",
        "    for ii in test_saved_index:\n",
        "        test_files.append(full_file_list[ii])\n",
        "    print('!!!! Using Test Indexed Files ONLY !!!!')\n",
        "fname = test_files[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_name = os.path.join(patch_dir,category_name,fname)\n",
        "\n",
        "print('testing file is ', test_name)\n",
        "print('test file is ',test_files[0])\n",
        "tfile = [test_files[0]]\n",
        "\n",
        "model_vgg16.load_state_dict(checkpoint)\n",
        "model_vgg16.eval()\n",
        "\n",
        "\n",
        "\n",
        "all_data = TestImageDataset(img_dir=patch_dir,\n",
        "                                category = full_category_name, \n",
        "                                file_count=len(tfile), #full_file_count,\n",
        "                                file_list = tfile, #flist, #new_file_list, #full_file_list, \n",
        "                                transform=None, \n",
        "                                target_transform=None)\n",
        "\n",
        "dataloader_all = DataLoader(all_data, batch_size=bsize,shuffle=True, num_workers=2)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader_all, 0):\n",
        "\n",
        "        inputs = data['image'].type(FloatTensor)\n",
        "\n",
        "        if (train_on_gpu):\n",
        "            inputs = inputs.to(dev) #, labels = inputs.to(dev), labels.to(dev)\n",
        "        # forward + backward + optimize\n",
        "        outputs = model_vgg16(inputs) #.permute(0, 1, 2, 3))\n",
        "        outputs=torch.flatten(outputs, start_dim=1)\n",
        "        #loss = criterion(outputs, labels)\n",
        "        y_pred_softmax = torch.log_softmax(outputs, dim = 1)\n",
        "        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
        "\n",
        "        print('Predicted Label is ', y_pred_tags)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT9hq8MnG3S6"
      },
      "source": [
        "##########################################################\n",
        "# Test against the test dataset to get accuracy results\n",
        "##########################################################\n",
        "run_test= 1\n",
        "if (run_test == 1):\n",
        "    #Mixing GPU and CPU model saves doesn't seem to map well yet\n",
        "    final_file = os.path.join(model_dir,model_name) #'vgg16_best_accuracy_81_EPOCH_79') #95_gpu_051821')#'vgg16_best_accuracy_97_gpu_final')\n",
        "    checkpoint = torch.load(final_file, map_location=torch.device('cpu'))\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    use_index = 1\n",
        "    if (use_index == 1):\n",
        "        #[val_index, test_index, file_list]\n",
        "        validation_saved_index,test_saved_index,saved_flist=pickle.load( open( index_file, \"rb\" )) \n",
        "        test_files=[]\n",
        "        for ii in test_saved_index:\n",
        "            test_files.append(full_file_list[ii])\n",
        "        print('!!!! Using Test Indexed Files ONLY !!!!')\n",
        "    else: #use full files      \n",
        "        bc_files=[]\n",
        "        for ii in full_file_list:\n",
        "            if (('Benign' in ii) or ('Cancer' in ii)):\n",
        "                bc_files.append(ii)\n",
        "                #print(ii)\n",
        "    \n",
        "    #new_file_list=[]\n",
        "    #for ii in b:\n",
        "    #    new_file_list.append(full_file_list[ii])\n",
        "\n",
        "\n",
        "    model_vgg16.load_state_dict(checkpoint)\n",
        "    model_vgg16.eval()\n",
        "\n",
        "\n",
        "    #load up with the pre-sized patch images\n",
        "    all_data = CustomImageDataset(img_dir=patch_dir,\n",
        "                                    category = full_category_name, \n",
        "                                    file_count=len(test_files), #full_file_count,\n",
        "                                    file_list = test_files, #flist, #new_file_list, #full_file_list, \n",
        "                                    transform=None, \n",
        "                                    target_transform=None)\n",
        "\n",
        "    dataloader_all = DataLoader(all_data, batch_size=bsize,shuffle=True, num_workers=2)#, \n",
        "\n",
        "    total_accuracy = []\n",
        "    total_precision = []\n",
        "    total_average_precision=[] \n",
        "    total_final_accuracy = []\n",
        "    total_final_precision = []\n",
        "    total_final_recall_sens = []\n",
        "    total_f1_score =[]\n",
        "    for epoch in range(0,1):\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(dataloader_all, 0):\n",
        "                print(i)\n",
        "                # get the inputs; data is a list of [inputs, labels]\n",
        "                #inputs, labels = data\n",
        "                inputs = data['image'].type(FloatTensor)\n",
        "                labels = data['label'] #.type(FloatTensor)\n",
        "\n",
        "                if (train_on_gpu):\n",
        "                    inputs, labels = inputs.to(dev), labels.to(dev)\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs = model_vgg16(inputs) #.permute(0, 1, 2, 3))\n",
        "\n",
        "                outputs=torch.flatten(outputs, start_dim=1)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "\n",
        "\n",
        "                #print(outputs)\n",
        "                y_pred_softmax = torch.log_softmax(outputs, dim = 1)\n",
        "                _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
        "                #print(y_pred_tags)\n",
        "                #print(labels)\n",
        "                correct_pred = (y_pred_tags == labels).float()\n",
        "                accuracy = correct_pred.sum() / len(correct_pred)\n",
        "                #accuracy = torch.round(accuracy)\n",
        "\n",
        "\n",
        "                #Get advanced metrics\n",
        "                average_precision, final_accuracy, final_precision, final_recall_sens, f1_score = calculate_metrics(labels,y_pred_tags)\n",
        "\n",
        "                total_average_precision.append(average_precision) \n",
        "                total_final_accuracy.append(final_accuracy)\n",
        "                total_final_precision.append(final_precision)\n",
        "                total_final_recall_sens.append(final_recall_sens)\n",
        "                total_f1_score.append(f1_score)\n",
        "\n",
        "                total_accuracy.append(accuracy)\n",
        "                \n",
        "                if (i%100 == 0):\n",
        "                    print('@ interim accuracy = ',i,  sum(total_accuracy)/len(total_accuracy))                \n",
        "                #print('-----#correct, training accuracy = ',correct_pred,accuracy)\n",
        "    print('Finished testing all data')\n",
        "    print('total accuracy = ', sum(total_accuracy)/len(total_accuracy))\n",
        "    test_p_file = os.path.join(tensorboard_dir,'test_metrics_debug.pickle')\n",
        "    pickle.dump([total_average_precision,total_final_accuracy, \n",
        "                 total_final_precision, total_final_recall_sens, \n",
        "                 total_f1_score, total_accuracy],\n",
        "                 open( test_p_file, \"wb\" ),protocol=5 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2z-ZQGXMLnT"
      },
      "source": [
        "def vgg_classify():\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}